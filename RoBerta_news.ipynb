{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from EDA import *\n",
    "from transformers import BertTokenizer, XLNetModel, XLNetTokenizer, XLNetForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6330</th>\n",
       "      <td>4490</td>\n",
       "      <td>State Department says it can't find emails fro...</td>\n",
       "      <td>The State Department told the Republican Natio...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331</th>\n",
       "      <td>8062</td>\n",
       "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
       "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6332</th>\n",
       "      <td>8622</td>\n",
       "      <td>Anti-Trump Protesters Are Tools of the Oligarc...</td>\n",
       "      <td>Anti-Trump Protesters Are Tools of the Oligar...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6333</th>\n",
       "      <td>4021</td>\n",
       "      <td>In Ethiopia, Obama seeks progress on peace, se...</td>\n",
       "      <td>ADDIS ABABA, Ethiopia —President Obama convene...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6334</th>\n",
       "      <td>4330</td>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6335 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              title  \\\n",
       "0           8476                       You Can Smell Hillary’s Fear   \n",
       "1          10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2           3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3          10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4            875   The Battle of New York: Why This Primary Matters   \n",
       "...          ...                                                ...   \n",
       "6330        4490  State Department says it can't find emails fro...   \n",
       "6331        8062  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...   \n",
       "6332        8622  Anti-Trump Protesters Are Tools of the Oligarc...   \n",
       "6333        4021  In Ethiopia, Obama seeks progress on peace, se...   \n",
       "6334        4330  Jeb Bush Is Suddenly Attacking Trump. Here's W...   \n",
       "\n",
       "                                                   text label  \n",
       "0     Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1     Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2     U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3     — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4     It's primary day in New York and front-runners...  REAL  \n",
       "...                                                 ...   ...  \n",
       "6330  The State Department told the Republican Natio...  REAL  \n",
       "6331  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...  FAKE  \n",
       "6332   Anti-Trump Protesters Are Tools of the Oligar...  FAKE  \n",
       "6333  ADDIS ABABA, Ethiopia —President Obama convene...  REAL  \n",
       "6334  Jeb Bush Is Suddenly Attacking Trump. Here's W...  REAL  \n",
       "\n",
       "[6335 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'./news.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = list(df['title'])\n",
    "text = list(df['text'])\n",
    "news = []\n",
    "for i in range(len(title)):\n",
    "    news.append(title[i] + '.\\n' + text[i])\n",
    "labels = list(df['label'])\n",
    "labels = [0 if i == 'FAKE' else 1 for i in labels]\n",
    "data = (news, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  5068\n",
      "test size:  1267\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "news_train, news_test, labels_train, labels_test = train_test_split(news, labels, test_size = 0.2)\n",
    "train_data = (news_train, labels_train)\n",
    "test_data = (news_test, labels_test)\n",
    "\n",
    "print('train size: ', len(train_data[0]))\n",
    "print('test size: ', len(test_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "state = np.random.get_state()\n",
    "np.random.shuffle(train_data[0])\n",
    "np.random.set_state(state)\n",
    "np.random.shuffle(train_data[1])\n",
    "\n",
    "\n",
    "state1 = np.random.get_state()\n",
    "np.random.shuffle(test_data[0])\n",
    "np.random.set_state(state1)\n",
    "np.random.shuffle(test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b03caf62b6641d0b80d4f40e943086c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbaa1f8b4f34a1284688363736a05d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd30d2cb06e94ebe81e02032afc7c2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Trump’s Camp Manager DESTROYS Hillary By Pointing Out 1 Thing We All Missed.\n",
      "Trump’s Camp Manager DESTROYS Hillary By Pointing Out 1 Thing We All Missed Posted on October 30, 2016 by Amanda Shea in Politics Share This Fox News Host (left), Trump’s Campaign Manager Kellyanne Conway (right) \n",
      "From the moment the FBI announced they’re investigating Hillary Clinton again, her ship of supporters quickly became lighter with people bailing out to protect themselves from being associated with her as a criminal. Donald Trump’s campaign couldn’t have asked for better vindication that the witch could soon be incarcerated, but the Republican candidate’s campaign manager just put the final nail in Clinton’s coffin. \n",
      "In the wake of this latest development that could save Western civilization, Kellyanne Conway went on Fox News to discuss it, but she delivered more than viewers were expecting. In a statement that didn’t just decimate Hillary’s campaign, she brought the entire Clinton family down with it in one perfectly stated sentence. \n",
      "When the FBI cracks a case back open that they had previously closed, there’s a legitimate reason to be terrified for the outcome of that investigation, and it’s almost guaranteed to end in incarceration. Former FBI Assistant Director James Kallstrom validated this when he said after the announcement that the final bomb is about to drop on Hillary within the next ten days. Adding to everything coming back to haunt Hillary since Friday, is what Conway said on Fox News: \n",
      "“What we are reminded of is this cloud of corruption that always follows Hillary Clinton and we are having this entire conversation about a renewed FBU investigation because Hillary Clinton did what she always does, put Hillary first…we can never get the stench and the stain of the Clintons off if us it turns out.” \n",
      "Call it karma or the combined forces of WikiLeaks, James O’Keefe, and now the FBI, who are all working together to bleach this “stain” from the White House, but things just aren’t looking good for Hillary who had it coming. Hopefully, the FBI who failed the American people after the first investigation that ended with her still being able to run for president, make good on that mistake by actually arresting her after round two. \n",
      "The investigation now is entirely different than the first, considering the fact that it was reopened. For that reason alone, Hillary shouldn’t feel comfortable about the outcome.\n",
      "Token IDs: tensor([    0,  7565,    17,    27,    29,  4746,  4827, 24566,  6997,   673,\n",
      "        14780,  5141,   870,  4937,   154,  2548,   112, 27033,   166,   404,\n",
      "         4523,   196,     4, 50118,  7565,    17,    27,    29,  4746,  4827,\n",
      "        24566,  6997,   673, 14780,  5141,   870,  4937,   154,  2548,   112,\n",
      "        27033,   166,   404,  4523,   196, 16171,    15,   779,   389,     6,\n",
      "          336,    30, 10641, 24326,    11, 16226,  1702,   152,  2063,   491,\n",
      "        10664,    36,  6960,   238,   140,    17,    27,    29, 11068,  4827,\n",
      "        12702,  7010,   858, 13896,    36,  4070,    43,  1437, 50118,  7605,\n",
      "            5,  1151,     5,  2448,   585,    51,    17,    27,   241,  3219,\n",
      "         5141,  2235,   456,     6,    69,  3627,     9,  2732,  1335,  1059,\n",
      "        13026,    19,    82,   741,  8459,    66,     7,  1744,  1235,    31,\n",
      "          145,  3059,    19,    69,    25,    10,  1837,     4,   807,   140,\n",
      "           17,    27,    29,   637,  1705,    17,    27,     2])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "train_input_ids = []\n",
    "train_attention_masks = []\n",
    "\n",
    "n=5068\n",
    "\n",
    "for sentence in train_data[0][:n]:\n",
    "\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text=sentence,  # the sentence to be encoded\n",
    "        add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "        max_length = 128,  # maximum length of a sentence\n",
    "        pad_to_max_length = True,\n",
    "#         padding='max_length',  # Add [PAD]s\n",
    "        return_attention_mask = True,  # Generate the attention mask\n",
    "        return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "    )\n",
    "\n",
    "    train_input_ids.append(encoded['input_ids'])\n",
    "#     print(encoded['input_ids'])\n",
    "    \n",
    "\n",
    "    train_attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "    \n",
    "train_input_ids = torch.cat(train_input_ids)\n",
    "train_attention_masks = torch.cat(train_attention_masks)\n",
    "train_labels = torch.tensor(train_data[1][:n])\n",
    "    \n",
    "\n",
    "# # Get the input IDs and attention mask in tensor format\n",
    "\n",
    "print('Original: ', train_data[0][0])\n",
    "print('Token IDs:', train_input_ids[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,561 training samples\n",
      "  507 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "\n",
    "dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  \n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset), \n",
    "            batch_size = batch_size \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef7ae3d0576416a9a9d25c8f29fab40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7196e7c1db4ba98fa82b7cbd35d2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\", \n",
    "    num_labels = 2, \n",
    "                   \n",
    "    output_attentions = False, .\n",
    "    output_hidden_states = False, \n",
    "    return_dict=False\n",
    ")\n",
    "\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
    "                )\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "  \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of    286.    Elapsed: 0:00:05.\n",
      "  Batch    80  of    286.    Elapsed: 0:00:11.\n",
      "  Batch   120  of    286.    Elapsed: 0:00:15.\n",
      "  Batch   160  of    286.    Elapsed: 0:00:21.\n",
      "  Batch   200  of    286.    Elapsed: 0:00:26.\n",
      "  Batch   240  of    286.    Elapsed: 0:00:31.\n",
      "  Batch   280  of    286.    Elapsed: 0:00:36.\n",
      "\n",
      "  Average training loss: 0.12\n",
      "  Training epcoh took: 0:00:37\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.99\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of    286.    Elapsed: 0:00:05.\n",
      "  Batch    80  of    286.    Elapsed: 0:00:10.\n",
      "  Batch   120  of    286.    Elapsed: 0:00:15.\n",
      "  Batch   160  of    286.    Elapsed: 0:00:20.\n",
      "  Batch   200  of    286.    Elapsed: 0:00:25.\n",
      "  Batch   240  of    286.    Elapsed: 0:00:30.\n",
      "  Batch   280  of    286.    Elapsed: 0:00:35.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epcoh took: 0:00:36\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 1.00\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of    286.    Elapsed: 0:00:05.\n",
      "  Batch    80  of    286.    Elapsed: 0:00:10.\n",
      "  Batch   120  of    286.    Elapsed: 0:00:15.\n",
      "  Batch   160  of    286.    Elapsed: 0:00:20.\n",
      "  Batch   200  of    286.    Elapsed: 0:00:25.\n",
      "  Batch   240  of    286.    Elapsed: 0:00:30.\n",
      "  Batch   280  of    286.    Elapsed: 0:00:36.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:00:36\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 1.00\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:01:51 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "seed_val = 42\n",
    "batch_size = 32\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "   \n",
    "    t0 = time.time()\n",
    "\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "       \n",
    "        optimizer.zero_grad()        \n",
    "\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "  \n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "       \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "     \n",
    "        optimizer.step()\n",
    "\n",
    "       \n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "  \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "       \n",
    "        with torch.no_grad():        \n",
    "            output = model(b_input_ids, \n",
    "                                   token_type_ids=None,\n",
    "                           attention_mask=b_input_mask)\n",
    "            \n",
    "\n",
    "        #total_eval_loss += loss.item()\n",
    "        logits = output[0]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    " \n",
    "    #avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "   \n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    #print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            #'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.22e-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0:00:37</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.83e-02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0:00:36</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.45e-03</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0:00:36</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                            \n",
       "1           1.22e-01           0.99       0:00:37         0:00:01\n",
       "2           1.83e-02           1.00       0:00:36         0:00:01\n",
       "3           4.45e-03           1.00       0:00:36         0:00:01"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "\n",
    "df_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 将数据集分完词后存储到列表中\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "\n",
    "n = 1267\n",
    "for sentence in test_data[0][:n]:\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text=sentence,  # the sentence to be encoded\n",
    "        add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "        max_length = 128,  # maximum length of a sentence\n",
    "        pad_to_max_length = True,\n",
    "#         padding='max_length',  # Add [PAD]s\n",
    "        return_attention_mask = True,  # Generate the attention mask\n",
    "        return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "    )\n",
    "  \n",
    "\n",
    "    test_input_ids.append(encoded['input_ids'])\n",
    "#     print(encoded['input_ids'])\n",
    "  \n",
    "  \n",
    "    test_attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "\n",
    "test_input_ids = torch.cat(test_input_ids)\n",
    "test_attention_masks = torch.cat(test_attention_masks)\n",
    "test_labels = torch.tensor(test_data[1][:n])\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "\n",
    "prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 1,267 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # 不需要计算梯度\n",
    "    with torch.no_grad():\n",
    "      # 前向传播，获取预测结果\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    #total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "\n",
    "\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "#avg_val_accuracy = flat_accuracy(predictions, true_labels)    \n",
    "#avg_val_accuracy = total_eval_accuracy / len(prediction_dataloader)\n",
    "#print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 615 of 1267 (48.54%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (sum(test_data[1]), len(test_data[1]), (sum(test_data[1]) / len(test_data[1]) * 100.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9913180741910024"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "pred_labels_i = []\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "    pred_labels_i += list(np.argmax(predictions[i], axis=1).flatten())\n",
    "\n",
    "    \n",
    "true_labels_list = []\n",
    "for i in test_labels:\n",
    "    true_labels_list.append(i.item())\n",
    "\n",
    "accuracy_score(pred_labels_i, true_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9840    0.9984    0.9911       615\n",
      "           0     0.9984    0.9847    0.9915       652\n",
      "\n",
      "    accuracy                         0.9913      1267\n",
      "   macro avg     0.9912    0.9915    0.9913      1267\n",
      "weighted avg     0.9914    0.9913    0.9913      1267\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'POS'), Text(0, 1.5, 'NEG')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print('Classification Report:') \n",
    "print(classification_report(true_labels_list, pred_labels_i, labels=[1,0], digits=4)) \n",
    "\n",
    "cm = confusion_matrix(true_labels_list, pred_labels_i, labels=[1,0]) \n",
    "ax= plt.subplot() \n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\") \n",
    "ax.set_title('Confusion Matrix') \n",
    "ax.set_xlabel('Predicted Labels') \n",
    "ax.set_ylabel('True Labels') \n",
    "ax.xaxis.set_ticklabels(['POS', 'NEG']) \n",
    "ax.yaxis.set_ticklabels(['POS', 'NEG']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
